<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.bluexiii.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="需求简介最近厂里有一个新闻采集类的需求，细节大体如下：  模拟登录一个内网网站（SSO） 抓取新闻（支持代理服务器的方式访问） 加工内容样式，以适配手机屏幕 将正文中的图片转存到自已的服务器，并替换标签中的url 图片存储服务器需要复用已有的FastDFS分布式文件系统 采集结果导入生产库 支持日志打印  初学Python3，正好用这个需求练练手，最后很惊讶的是只用200多行代码就实现了，如果换成">
<meta property="og:type" content="article">
<meta property="og:title" content="使用Python3编写一个爬虫">
<meta property="og:url" content="http://www.bluexiii.com/201702/%E4%BD%BF%E7%94%A8Python3%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AA%E7%88%AC%E8%99%AB/index.html">
<meta property="og:site_name" content="BlueXIII&#39;s Blog">
<meta property="og:description" content="需求简介最近厂里有一个新闻采集类的需求，细节大体如下：  模拟登录一个内网网站（SSO） 抓取新闻（支持代理服务器的方式访问） 加工内容样式，以适配手机屏幕 将正文中的图片转存到自已的服务器，并替换标签中的url 图片存储服务器需要复用已有的FastDFS分布式文件系统 采集结果导入生产库 支持日志打印  初学Python3，正好用这个需求练练手，最后很惊讶的是只用200多行代码就实现了，如果换成">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://bluexiii.oss-cn-beijing.aliyuncs.com/p74nq.jpg">
<meta property="og:image" content="https://bluexiii.oss-cn-beijing.aliyuncs.com/o552x.jpg">
<meta property="article:published_time" content="2017-02-20T07:06:10.000Z">
<meta property="article:modified_time" content="2021-01-26T01:44:33.904Z">
<meta property="article:author" content="BlueXIII">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://bluexiii.oss-cn-beijing.aliyuncs.com/p74nq.jpg">

<link rel="canonical" href="http://www.bluexiii.com/201702/%E4%BD%BF%E7%94%A8Python3%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AA%E7%88%AC%E8%99%AB/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>使用Python3编写一个爬虫 | BlueXIII's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">BlueXIII's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">热爱技术,持续学习</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://www.bluexiii.com/201702/%E4%BD%BF%E7%94%A8Python3%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AA%E7%88%AC%E8%99%AB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="BlueXIII">
      <meta itemprop="description" content="IT技术类文章、笔记分享">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BlueXIII's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          使用Python3编写一个爬虫
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-02-20 15:06:10" itemprop="dateCreated datePublished" datetime="2017-02-20T15:06:10+08:00">2017-02-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-01-26 09:44:33" itemprop="dateModified" datetime="2021-01-26T09:44:33+08:00">2021-01-26</time>
              </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="需求简介"><a href="#需求简介" class="headerlink" title="需求简介"></a>需求简介</h2><p>最近厂里有一个新闻采集类的需求，细节大体如下：</p>
<ol>
<li>模拟登录一个内网网站（SSO）</li>
<li>抓取新闻（支持代理服务器的方式访问）</li>
<li>加工内容样式，以适配手机屏幕</li>
<li>将正文中的图片转存到自已的服务器，并替换<img>标签中的url</li>
<li>图片存储服务器需要复用已有的FastDFS分布式文件系统</li>
<li>采集结果导入生产库</li>
<li>支持日志打印</li>
</ol>
<p>初学Python3，正好用这个需求练练手，最后很惊讶的是只用200多行代码就实现了，如果换成Java的话大概需要1200行吧。果然应了那句老话：<strong>人生苦短，我用Python</strong>  </p>
<h2 id="登录页面抓包"><a href="#登录页面抓包" class="headerlink" title="登录页面抓包"></a>登录页面抓包</h2><p>第一步当然是抓包，然后再根据抓到的内容，模拟进行HTTP请求。  </p>
<p>常用的抓包工具，有Mac下的Charles和Windows下的Fiddler。<br>它们的原理都是在本机开一个HTTP或SOCKS代理服务器端口，然后将浏览器的代理服务器设置成这个端口，这样浏览器中所有的HTTP请求都会先经过抓包工具记录下来了。  </p>
<ul>
<li><p>Chrales<br><a target="_blank" rel="noopener" href="https://www.charlesproxy.com/">https://www.charlesproxy.com</a><br><img src="https://bluexiii.oss-cn-beijing.aliyuncs.com/p74nq.jpg"></p>
</li>
<li><p>Fiddler<br><a target="_blank" rel="noopener" href="http://www.telerik.com/fiddler">http://www.telerik.com/fiddler</a><br><img src="https://bluexiii.oss-cn-beijing.aliyuncs.com/o552x.jpg"></p>
</li>
</ul>
<p>这里推荐尽量使用Fiddler，原因是Charles对于cookie的展示是有bug的，举个例子，真实情况：请求A返回了LtpaToken这个cookie，请求B中返回了sid这个cookie。但在Charles中的展示是：请求A中已经同时返回了LtpaToken和sid两个cookie，这就很容易误导人了。<br>另外Fiddler现在已经有了Linux的Beta版本，貌似是用类似wine的方式实现的。  </p>
<p>如果网站使用了单点登录，可能会涉及到手工生成cookie。所以不仅需要分析每一条HTTP请求的request和response，以及带回来的cookie，还要对页面中的javascript进行分析，看一下是如何生成cookie的。  </p>
<h2 id="模拟登录"><a href="#模拟登录" class="headerlink" title="模拟登录"></a>模拟登录</h2><p>将页面分析完毕之后，就可以进行模拟HTTP请求了。<br>这里有两个非常好用的第三方库， <strong>request</strong> 和 <strong>BeautifulSoup</strong></p>
<p><strong>requests</strong> 库是用来代替urllib的，可以非常人性化的的生成HTTP请求，模拟session以及伪造cookie更是方便。<br><strong>BeautifulSoup</strong> 用来代替re模块，进行HTML内容解析，可以用tag, class, id来定位想要提取的内容，也支持正则表达式等。  </p>
<p>具体的使用方式直接看官方文档就可以了，写的非常详细，这里直接给出地址：<br><a target="_blank" rel="noopener" href="http://cn.python-requests.org/zh_CN/latest/">requests官方文档</a><br><a target="_blank" rel="noopener" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/">BeautifulSoup官方文档</a>  </p>
<p>通过pip3来安装这两个模块:  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install python3-pip</span><br><span class="line">sudo pip3 install requests</span><br><span class="line">sudo pip3 install beautifulsoup4</span><br></pre></td></tr></table></figure>
<p>导入模块：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from bs4 import BeautifulSoup</span><br></pre></td></tr></table></figure>

<p>模拟登录：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">def sso_login():</span><br><span class="line">    # 调用单点登录工号认证页面</span><br><span class="line">    response &#x3D; session.post(const.SSO_URL,</span><br><span class="line">                            data&#x3D;&#123;&#39;login&#39;: const.LOGIN_USERNAME, &#39;password&#39;: const.LOGIN_PASSWORD, &#39;appid&#39;: &#39;np000&#39;&#125;)</span><br><span class="line"></span><br><span class="line">    # 分析页面，取token及ltpa</span><br><span class="line">    soup &#x3D; BeautifulSoup(response.text, &#39;html.parser&#39;)</span><br><span class="line">    token &#x3D; soup.form.input.get(&#39;value&#39;)</span><br><span class="line">    ltpa &#x3D; soup.form.input.input.input.get(&#39;value&#39;)</span><br><span class="line">    ltpa_value &#x3D; ltpa.split(&#39;;&#39;)[0].split(&#39;&#x3D;&#39;, 1)[1]</span><br><span class="line"></span><br><span class="line">    # 手工设置Cookie</span><br><span class="line">    session.cookies.set(&#39;LtpaToken&#39;, ltpa_value, domain&#x3D;&#39;unicom.local&#39;, path&#x3D;&#39;&#x2F;&#39;)</span><br><span class="line"></span><br><span class="line">    # 调用云门户登录页面(2次）</span><br><span class="line">    payload &#x3D; &#123;&#39;token&#39;: token&#125;</span><br><span class="line">    session.post(const.LOGIN_URL, data&#x3D;payload, proxies&#x3D;const.PROXIES)</span><br><span class="line">    response &#x3D; session.post(const.LOGIN_URL, data&#x3D;payload, proxies&#x3D;const.PROXIES)</span><br><span class="line">    if response.text &#x3D;&#x3D; &quot;success&quot;:</span><br><span class="line">        logging.info(&quot;登录成功&quot;)</span><br><span class="line">        return True</span><br><span class="line">    else:</span><br><span class="line">        logging.info(&quot;登录失败&quot;)</span><br><span class="line">        return False</span><br></pre></td></tr></table></figure>
<p>这里用到了BeautifulSoup进行HTML解析，取出页面中的token、ltpa等字段。<br>然后使用session.cookies.set伪造了一个cookie，注意其中的domain参数，设置成1级域名。<br>然后用这个session，去调用网站页面，换回sid这个token。并可以根据页面的返回信息，来简单判断一下成功还是失败。</p>
<h2 id="列表页面抓取"><a href="#列表页面抓取" class="headerlink" title="列表页面抓取"></a>列表页面抓取</h2><p>登录成功之后，接下来的列表页面抓取就要简单的多了，不考虑分页的话，直接取一个list出来遍历即可。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def capture_list(list_url):</span><br><span class="line">    response &#x3D; session.get(list_url, proxies&#x3D;const.PROXIES)</span><br><span class="line">    response.encoding &#x3D; &quot;UTF-8&quot;</span><br><span class="line">    soup &#x3D; BeautifulSoup(response.text, &#39;html.parser&#39;)</span><br><span class="line">    news_list &#x3D; soup.find(&#39;div&#39;, &#39;xinwen_list&#39;).find_all(&#39;a&#39;)</span><br><span class="line">    news_list.reverse()</span><br><span class="line">    logging.info(&quot;开始采集&quot;)</span><br><span class="line">    for news_archor in news_list:</span><br><span class="line">        news_cid &#x3D; news_archor.attrs[&#39;href&#39;].split(&#39;&#x3D;&#39;)[1]</span><br><span class="line">        capture_content(news_cid)</span><br><span class="line">    logging.info(&quot;结束采集&quot;)</span><br></pre></td></tr></table></figure>
<p>这里使用了<code>response.encoding = &quot;UTF-8&quot;</code>来手工解决乱码问题。  </p>
<h2 id="新联页面抓取"><a href="#新联页面抓取" class="headerlink" title="新联页面抓取"></a>新联页面抓取</h2><p>新闻页面抓取，涉及到插临时表，这里没有使用每三方库，直接用SQL方式插入。<br>其中涉及到样式处理与图片转存，另写一个模块<code>pconvert</code>来实现。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">def capture_content(news_cid):</span><br><span class="line">    # 建立DB连接</span><br><span class="line">    conn &#x3D; mysql.connector.connect(user&#x3D;const.DB_USERNAME, password&#x3D;const.DB_PASSWORD, host&#x3D;const.DB_HOST,</span><br><span class="line">                                   port&#x3D;const.DB_PORT, database&#x3D;const.DB_DATABASE)</span><br><span class="line">    cursor &#x3D; conn.cursor()</span><br><span class="line"></span><br><span class="line">    # 判断是否已存在</span><br><span class="line">    cursor.execute(&#39;select count(*) from material_prepare where news_cid &#x3D; %s&#39;, (news_cid,))</span><br><span class="line">    news_count &#x3D; cursor.fetchone()[0]</span><br><span class="line">    if news_count &gt; 0:</span><br><span class="line">        logging.info(&quot;采集&quot; + news_cid + &#39;:已存在&#39;)</span><br><span class="line">    else:</span><br><span class="line">        logging.info(&quot;采集&quot; + news_cid + &#39;:新增&#39;)</span><br><span class="line">        news_url &#x3D; const.NEWS_BASE_URL + news_cid</span><br><span class="line">        response &#x3D; session.post(news_url, proxies&#x3D;const.PROXIES)</span><br><span class="line">        response.encoding &#x3D; &quot;UTF-8&quot;</span><br><span class="line">        soup &#x3D; BeautifulSoup(response.text, &#39;html.parser&#39;)</span><br><span class="line">        # logging.info(soup)</span><br><span class="line">        news_title &#x3D; soup.h3.text.strip()[:64]</span><br><span class="line">        news_brief &#x3D; soup.find(&#39;div&#39;, &#39;brief&#39;).p.text.strip()[:100]</span><br><span class="line">        news_author &#x3D; soup.h5.span.a.text.strip()[:100]</span><br><span class="line">        news_content &#x3D; soup.find(&#39;table&#39;, &#39;unis_detail_content&#39;).tr.td.prettify()[66:-7].strip()</span><br><span class="line">        # 样式处理</span><br><span class="line">        news_content &#x3D; pconvert.convert_style(news_content)</span><br><span class="line">        # 将图片转存至DFS并替换URL</span><br><span class="line">        news_content &#x3D; pconvert.convert_img(news_content)</span><br><span class="line">        # 入表</span><br><span class="line">        cursor.execute(</span><br><span class="line">            &#39;INSERT INTO material_prepare (news_cid, title, author, summary, content, add_time, status)  VALUES  (%s, %s, %s, %s, %s, now(), &quot;0&quot;)&#39;</span><br><span class="line">            , [news_cid, news_title, news_author, news_brief, news_content])</span><br><span class="line">    # 提交</span><br><span class="line">    conn.commit()</span><br><span class="line">    cursor.close()</span><br></pre></td></tr></table></figure>
<h2 id="样式处理"><a href="#样式处理" class="headerlink" title="样式处理"></a>样式处理</h2><p>文本样式处理，还是要用到BeautifulSoup，因为原始站点上的新闻内容样式是五花八门的，根据实际情况，一边写一个test函数来生成文本，一边在浏览器上慢慢调试。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def convert_style(rawtext):</span><br><span class="line">    newtext &#x3D; &#39;&lt;div style&#x3D;&quot;margin-left: 0px; margin-right:0px; letter-spacing: 1px; word-spacing:2px;line-height: 1.7em; font-size:18px;text-align:justify; text-justify:inter-ideograph&quot;&gt;&#39; \</span><br><span class="line">              + rawtext + &#39;&lt;&#x2F;div&gt;&#39;</span><br><span class="line">    newtext &#x3D; newtext.replace(&#39; align&#x3D;&quot;center&quot;&#39;, &#39;&#39;)</span><br><span class="line">    soup &#x3D; BeautifulSoup(newtext, &#39;html.parser&#39;)</span><br><span class="line">    img_tags &#x3D; soup.find_all(&quot;img&quot;)</span><br><span class="line">    for img_tag in img_tags:</span><br><span class="line">        del img_tag.parent[&#39;style&#39;]</span><br><span class="line">    return soup.prettify()</span><br></pre></td></tr></table></figure>


<h2 id="图片转存至DFS"><a href="#图片转存至DFS" class="headerlink" title="图片转存至DFS"></a>图片转存至DFS</h2><p>因为原始站点是在内网中的，采集下来的HTML中，<img>标签的地址是内网地址，所以在公网中是展现不出来的，需要将图片转存，并用新的URL替换原有的URL。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def convert_img(rawtext):</span><br><span class="line">    soup &#x3D; BeautifulSoup(rawtext, &#39;html.parser&#39;)</span><br><span class="line">    img_tags &#x3D; soup.find_all(&quot;img&quot;)</span><br><span class="line">    for img_tag in img_tags:</span><br><span class="line">        raw_img_url &#x3D; img_tag[&#39;src&#39;]</span><br><span class="line">        dfs_img_url &#x3D; convert_url(raw_img_url)</span><br><span class="line">        img_tag[&#39;src&#39;] &#x3D; dfs_img_url</span><br><span class="line">        del img_tag[&#39;style&#39;]</span><br><span class="line">    return soup.prettify()</span><br></pre></td></tr></table></figure>
<p>图片转存最简单的方式是保存成本地的文件，然后再通过nginx或httpd服务将图片开放出去：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pic_name &#x3D; raw_img_url.split(&#39;&#x2F;&#39;)[-1]</span><br><span class="line">pic_path &#x3D; TMP_PATH + &#39;&#x2F;&#39; + pic_name</span><br><span class="line">with open(pic_path, &#39;wb&#39;) as pic_file:</span><br><span class="line">   pic_file.write(pic_content)</span><br></pre></td></tr></table></figure>
<p>但这里我们需要复用已有的FastDFS分布式文件系统，要用到它的一个客户端的库<code>fdfs_client-py</code><br>fdfs_client-py不能直接使用pip3安装，需要直接使用一个python3版的源码，并手工修改其中代码。操作过程如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;jefforeilly&#x2F;fdfs_client-py.git</span><br><span class="line">cd dfs_client-py</span><br><span class="line">vi .&#x2F;fdfs_client&#x2F;storage_client.py</span><br><span class="line">将第12行 from fdfs_client.sendfile import * 注释掉</span><br><span class="line">python3 setup.py install</span><br><span class="line"></span><br><span class="line">sudo pip3 install mutagen</span><br></pre></td></tr></table></figure>
<p>客户端的使用上没有什么特别的，直接调用<code>upload_by_buffer</code>，传一个图片的buffer进去就可以了，成功后会返回自动生成的文件名。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from fdfs_client.client import *</span><br><span class="line">dfs_client &#x3D; Fdfs_client(&#39;conf&#x2F;dfs.conf&#39;)</span><br><span class="line">def convert_url(raw_img_url):</span><br><span class="line">    response &#x3D; requests.get(raw_img_url, proxies&#x3D;const.PROXIES)</span><br><span class="line">    pic_buffer &#x3D; response.content</span><br><span class="line">    pic_ext &#x3D; raw_img_url.split(&#39;.&#39;)[-1]</span><br><span class="line">    response &#x3D; dfs_client.upload_by_buffer(pic_buffer, pic_ext)</span><br><span class="line">    dfs_img_url &#x3D; const.DFS_BASE_URL + &#39;&#x2F;&#39; + response[&#39;Remote file_id&#39;]</span><br><span class="line">    return dfs_img_url</span><br></pre></td></tr></table></figure>
<p>其中dfs.conf文件中，主要就是配置一下 <strong>tracker_server</strong></p>
<h2 id="日志处理"><a href="#日志处理" class="headerlink" title="日志处理"></a>日志处理</h2><p>这里使用配置文件的方式处理日志，类似JAVA中的log4j吧，首先新建一个<code>log.conf</code>：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[loggers]</span><br><span class="line">keys&#x3D;root</span><br><span class="line"></span><br><span class="line">[handlers]</span><br><span class="line">keys&#x3D;stream_handler,file_handler</span><br><span class="line"></span><br><span class="line">[formatters]</span><br><span class="line">keys&#x3D;formatter</span><br><span class="line"></span><br><span class="line">[logger_root]</span><br><span class="line">level&#x3D;DEBUG</span><br><span class="line">handlers&#x3D;stream_handler,file_handler</span><br><span class="line"></span><br><span class="line">[handler_stream_handler]</span><br><span class="line">class&#x3D;StreamHandler</span><br><span class="line">level&#x3D;DEBUG</span><br><span class="line">formatter&#x3D;formatter</span><br><span class="line">args&#x3D;(sys.stderr,)</span><br><span class="line"></span><br><span class="line">[handler_file_handler]</span><br><span class="line">class&#x3D;FileHandler</span><br><span class="line">level&#x3D;DEBUG</span><br><span class="line">formatter&#x3D;formatter</span><br><span class="line">args&#x3D;(&#39;logs&#x2F;pspider.log&#39;,&#39;a&#39;,&#39;utf8&#39;)</span><br><span class="line"></span><br><span class="line">[formatter_formatter]</span><br><span class="line">format&#x3D;%(asctime)s %(name)-12s %(levelname)-8s %(message)s</span><br></pre></td></tr></table></figure>
<p>这里通过配置handlers，可以同时将日志打印到stderr和文件。<br>注意<code>args=(&#39;logs/pspider.log&#39;,&#39;a&#39;,&#39;utf8&#39;)</code> 这一行，用来解决文本文件中的中文乱码问题。  </p>
<p>日志初始化：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import logging</span><br><span class="line">from logging.config import fileConfig</span><br><span class="line"></span><br><span class="line">fileConfig(&#39;conf&#x2F;log.conf&#39;)</span><br></pre></td></tr></table></figure>
<p>日志打印：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logging.info(&quot;test&quot;)</span><br></pre></td></tr></table></figure>
<h2 id="完整源码"><a href="#完整源码" class="headerlink" title="完整源码"></a>完整源码</h2><p>到此为止，就是如何用Python3写一个爬虫的全部过程了。<br>采集不同的站点，肯定是要有不同的处理，但方法都是大同小异。<br>最后，将源码做了部分裁剪，分享在了GitHub上。<br><a target="_blank" rel="noopener" href="https://github.com/xiiiblue/pspider">https://github.com/xiiiblue/pspider</a><br>最后，将源码做了部分裁剪，分享在了GitLab上。<br><a target="_blank" rel="noopener" href="http://git.si-tech.com.cn/guolei/pspider">http://git.si-tech.com.cn/guolei/pspider</a></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/201702/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7Redmine%E6%93%8D%E4%BD%9C%E6%89%8B%E5%86%8C/" rel="prev" title="项目管理工具Redmine操作手册">
      <i class="fa fa-chevron-left"></i> 项目管理工具Redmine操作手册
    </a></div>
      <div class="post-nav-item">
    <a href="/201702/Python3%E4%B8%AA%E4%BA%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="next" title="Python3个人学习笔记">
      Python3个人学习笔记 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9C%80%E6%B1%82%E7%AE%80%E4%BB%8B"><span class="nav-number">1.</span> <span class="nav-text">需求简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%99%BB%E5%BD%95%E9%A1%B5%E9%9D%A2%E6%8A%93%E5%8C%85"><span class="nav-number">2.</span> <span class="nav-text">登录页面抓包</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95"><span class="nav-number">3.</span> <span class="nav-text">模拟登录</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%97%E8%A1%A8%E9%A1%B5%E9%9D%A2%E6%8A%93%E5%8F%96"><span class="nav-number">4.</span> <span class="nav-text">列表页面抓取</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B0%E8%81%94%E9%A1%B5%E9%9D%A2%E6%8A%93%E5%8F%96"><span class="nav-number">5.</span> <span class="nav-text">新联页面抓取</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%B7%E5%BC%8F%E5%A4%84%E7%90%86"><span class="nav-number">6.</span> <span class="nav-text">样式处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%BE%E7%89%87%E8%BD%AC%E5%AD%98%E8%87%B3DFS"><span class="nav-number">7.</span> <span class="nav-text">图片转存至DFS</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%97%A5%E5%BF%97%E5%A4%84%E7%90%86"><span class="nav-number">8.</span> <span class="nav-text">日志处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%8C%E6%95%B4%E6%BA%90%E7%A0%81"><span class="nav-number">9.</span> <span class="nav-text">完整源码</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">BlueXIII</p>
  <div class="site-description" itemprop="description">IT技术类文章、笔记分享</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">231</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/xiiiblue" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;xiiiblue" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/bluexiii" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;bluexiii" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">鲁ICP备18050487号-1 </a>
  </div>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">BlueXIII</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
